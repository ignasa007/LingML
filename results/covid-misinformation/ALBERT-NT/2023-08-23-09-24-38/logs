DATASET = covid-misinformation
COVID WORDS = covid 19, covid-19, covid19, covid, corona virus, coronavirus, corona
TOKENS = [URL], [HASHTAG], [MENTION], [EMOJI], [COVID]
ADD NEW TOKENS = True
MAX LENGTH = 128
BATCH SIZE = 64
MODEL = albert-base-v2
LEARNING RATE = 1e-05
TEST EVERY = 10000
SAVE EVERY = None

2023-08-23-09-24-38: Loading and pre-processing datasets...
2023-08-23-09-26-41: Finished pre-processing datasets.

2023-08-23-09-26-41: Tokenizing datasets...
2023-08-23-09-29-50: Finished tokenizing datasets.

2023-08-23-09-29-50: Preparing data-loaders...
2023-08-23-09-29-50: Finished preparing data-loaders.

2023-08-23-09-29-50: Loading and preparing model...
2023-08-23-09-29-52: Finshed preparing model.

2023-08-23-09-29-52: Starting training...

2023-08-23-10-22-18: Training (last 10000 batches): accuracy = 0.872891, f1-score = 0.899143, loss = 2944.739167
2023-08-23-10-28-37: Validation (total 3214 batches): accuracy = 0.899097, f1-score = 0.920755, loss = 758.948975
2023-08-23-10-28-37: Finished batch 10000.

2023-08-23-11-21-11: Training (last 10000 batches): accuracy = 0.911291, f1-score = 0.928677, loss = 2075.829884
2023-08-23-11-27-32: Validation (total 3214 batches): accuracy = 0.919128, f1-score = 0.934428, loss = 614.839172
2023-08-23-11-27-32: Finished batch 20000.

2023-08-23-12-20-15: Training (last 10000 batches): accuracy = 0.922591, f1-score = 0.937554, loss = 1805.518027
2023-08-23-12-26-34: Validation (total 3214 batches): accuracy = 0.923785, f1-score = 0.939462, loss = 574.104553
2023-08-23-12-26-34: Finished batch 30000.

2023-08-23-13-19-04: Training (last 10000 batches): accuracy = 0.931779, f1-score = 0.944845, loss = 1575.297886
2023-08-23-13-25-23: Validation (total 3214 batches): accuracy = 0.928910, f1-score = 0.942939, loss = 537.488403
2023-08-23-13-25-23: Finished batch 40000.

2023-08-23-14-08-35: Training (last 10000 batches): accuracy = 0.933529, f1-score = 0.946093, loss = 1533.096231
2023-08-23-14-14-55: Validation (total 3214 batches): accuracy = 0.928088, f1-score = 0.941264, loss = 535.985535
2023-08-23-14-14-55: Finished batch 48207.

