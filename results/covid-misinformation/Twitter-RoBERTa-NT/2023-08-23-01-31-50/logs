DATASET = covid-misinformation
COVID WORDS = covid 19, covid-19, covid19, covid, corona virus, coronavirus, corona
TOKENS = [URL], [HASHTAG], [MENTION], [EMOJI], [COVID]
ADD NEW TOKENS = True
MAX LENGTH = 286
BATCH SIZE = 64
MODEL = twitter-roberta-base-sentiment-latest
LEARNING RATE = 1e-05
TEST EVERY = 10000
SAVE EVERY = None

2023-08-23-01-31-50: Loading and pre-processing datasets...
2023-08-23-01-33-55: Finished pre-processing datasets.

2023-08-23-01-33-55: Tokenizing datasets...
2023-08-23-01-37-04: Finished tokenizing datasets.

2023-08-23-01-37-04: Preparing data-loaders...
2023-08-23-01-37-04: Finished preparing data-loaders.

2023-08-23-01-37-04: Loading and preparing model...
2023-08-23-01-37-07: Finshed preparing model.

2023-08-23-01-37-07: Starting training...

2023-08-23-05-01-08: Training (last 10000 batches): accuracy = 0.896036, f1-score = 0.916636, loss = 2418.981936
2023-08-23-05-28-20: Validation (total 3214 batches): accuracy = 0.920387, f1-score = 0.936636, loss = 591.656250
2023-08-23-05-28-20: Finished batch 10000.

2023-08-23-08-58-29: Training (last 10000 batches): accuracy = 0.923848, f1-score = 0.938622, loss = 1781.085871
2023-08-23-09-25-35: Validation (total 3214 batches): accuracy = 0.929177, f1-score = 0.943103, loss = 536.827087
2023-08-23-09-25-35: Finished batch 20000.

2023-08-23-12-41-06: Training (last 10000 batches): accuracy = 0.931478, f1-score = 0.944585, loss = 1586.686645
2023-08-23-13-03-23: Validation (total 3214 batches): accuracy = 0.933383, f1-score = 0.946202, loss = 518.250977
2023-08-23-13-03-23: Finished batch 30000.

2023-08-23-15-46-13: Training (last 10000 batches): accuracy = 0.939809, f1-score = 0.951148, loss = 1394.765010
2023-08-23-16-08-43: Validation (total 3214 batches): accuracy = 0.933164, f1-score = 0.945345, loss = 538.170837
2023-08-23-16-08-43: Finished batch 40000.

2023-08-23-18-55-24: Training (last 10000 batches): accuracy = 0.940904, f1-score = 0.951977, loss = 1354.395931
2023-08-23-19-20-50: Validation (total 3214 batches): accuracy = 0.936305, f1-score = 0.948354, loss = 493.336700
2023-08-23-19-20-50: Finished batch 48207.

