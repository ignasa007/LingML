DATASET = aaai-constraint-covid
COVID WORDS = covid 19, covid-19, covid19, covid, corona virus, coronavirus, corona
TOKENS = [URL], [HASHTAG], [MENTION], [EMOJI], [COVID]
ADD NEW TOKENS = False
MAX LENGTH = 128
BATCH SIZE = 12
MODEL = albert-base-v2
LEARNING RATE = 1e-05
TEST EVERY = 600
SAVE EVERY = None

2023-10-28-08-30-35: Loading and pre-processing datasets...
2023-10-28-08-30-36: Finished pre-processing datasets.

2023-10-28-08-30-36: Tokenizing datasets...
2023-10-28-08-30-40: Finished tokenizing datasets.

2023-10-28-08-30-40: Preparing data-loaders...
2023-10-28-08-30-40: Finished preparing data-loaders.

2023-10-28-08-30-40: Loading and preparing model...
2023-10-28-08-30-41: Finshed preparing model.

2023-10-28-08-30-41: Starting training...

2023-10-28-08-31-26: Training (last 600 batches): accuracy = 0.917639, f1-score = 0.921840, loss = 125.902282
2023-10-28-08-31-31: Validation (total 179 batches): accuracy = 0.953271, f1-score = 0.955752, loss = 24.731220
2023-10-28-08-31-36: Testing (total 179 batches): accuracy = 0.954206, f1-score = 0.956904, loss = 23.392557
2023-10-28-08-31-36: Finished batch 600.

2023-10-28-08-32-22: Training (last 600 batches): accuracy = 0.970000, f1-score = 0.971353, loss = 53.496992
2023-10-28-08-32-27: Validation (total 179 batches): accuracy = 0.954673, f1-score = 0.956946, loss = 23.255035
2023-10-28-08-32-32: Testing (total 179 batches): accuracy = 0.962150, f1-score = 0.964112, loss = 19.613134
2023-10-28-08-32-32: Finished batch 1200.

2023-10-28-08-33-18: Training (last 600 batches): accuracy = 0.981667, f1-score = 0.982654, loss = 33.340539
2023-10-28-08-33-23: Validation (total 179 batches): accuracy = 0.956542, f1-score = 0.957592, loss = 24.555607
2023-10-28-08-33-28: Testing (total 179 batches): accuracy = 0.962150, f1-score = 0.963031, loss = 22.149075
2023-10-28-08-33-28: Finished batch 1800.

2023-10-28-08-34-14: Training (last 600 batches): accuracy = 0.988750, f1-score = 0.989213, loss = 20.708585
2023-10-28-08-34-19: Validation (total 179 batches): accuracy = 0.964953, f1-score = 0.967263, loss = 26.186100
2023-10-28-08-34-24: Testing (total 179 batches): accuracy = 0.963551, f1-score = 0.965879, loss = 23.496298
2023-10-28-08-34-24: Finished batch 2400.

2023-10-28-08-35-10: Training (last 600 batches): accuracy = 0.992222, f1-score = 0.992575, loss = 14.565577
2023-10-28-08-35-15: Validation (total 179 batches): accuracy = 0.971963, f1-score = 0.973214, loss = 18.863499
2023-10-28-08-35-19: Testing (total 179 batches): accuracy = 0.963551, f1-score = 0.964928, loss = 22.026472
2023-10-28-08-35-19: Finished batch 3000.

2023-10-28-08-36-05: Training (last 600 batches): accuracy = 0.992917, f1-score = 0.993258, loss = 12.536138
2023-10-28-08-36-10: Validation (total 179 batches): accuracy = 0.965888, f1-score = 0.967541, loss = 21.927919
2023-10-28-08-36-15: Testing (total 179 batches): accuracy = 0.965888, f1-score = 0.967396, loss = 20.531433
2023-10-28-08-36-15: Finished batch 3600.

2023-10-28-08-37-00: Training (last 600 batches): accuracy = 0.996528, f1-score = 0.996675, loss = 5.949712
2023-10-28-08-37-05: Validation (total 179 batches): accuracy = 0.966822, f1-score = 0.968346, loss = 26.691671
2023-10-28-08-37-10: Testing (total 179 batches): accuracy = 0.967290, f1-score = 0.968610, loss = 25.185308
2023-10-28-08-37-10: Finished batch 4200.

2023-10-28-08-37-56: Training (last 600 batches): accuracy = 0.992917, f1-score = 0.993251, loss = 12.140802
2023-10-28-08-38-00: Validation (total 179 batches): accuracy = 0.966355, f1-score = 0.968476, loss = 26.227127
2023-10-28-08-38-05: Testing (total 179 batches): accuracy = 0.961682, f1-score = 0.964255, loss = 23.316248
2023-10-28-08-38-05: Finished batch 4800.

2023-10-28-08-38-47: Training (last 600 batches): accuracy = 0.995000, f1-score = 0.995234, loss = 8.514374
2023-10-28-08-38-52: Validation (total 179 batches): accuracy = 0.967757, f1-score = 0.969510, loss = 20.710730
2023-10-28-08-38-57: Testing (total 179 batches): accuracy = 0.971495, f1-score = 0.972829, loss = 20.215885
2023-10-28-08-38-57: Finished batch 5350.

