DATASET = aaai-constraint-covid
COVID WORDS = covid 19, covid-19, covid19, covid, corona virus, coronavirus, corona
TOKENS = [URL], [HASHTAG], [MENTION], [EMOJI], [COVID]
ADD NEW TOKENS = False
MAX LENGTH = 128
BATCH SIZE = 12
MODEL = albert-base-v2
LEARNING RATE = 1e-05
TEST EVERY = 600
SAVE EVERY = None

2023-10-30-06-21-19: Loading and pre-processing datasets...
2023-10-30-06-21-21: Finished pre-processing datasets.

2023-10-30-06-21-21: Tokenizing datasets...
2023-10-30-06-21-24: Finished tokenizing datasets.

2023-10-30-06-21-24: Preparing data-loaders...
2023-10-30-06-21-24: Finished preparing data-loaders.

2023-10-30-06-21-24: Loading and preparing model...
2023-10-30-06-21-25: Finshed preparing model.

2023-10-30-06-21-25: Starting training...

2023-10-30-06-22-11: Training (last 600 batches): accuracy = 0.913750, f1-score = 0.918408, loss = 134.318898
2023-10-30-06-22-16: Validation (total 179 batches): accuracy = 0.952804, f1-score = 0.954931, loss = 25.194897
2023-10-30-06-22-21: Testing (total 179 batches): accuracy = 0.958411, f1-score = 0.960392, loss = 22.298868
2023-10-30-06-22-21: Finished batch 600.

2023-10-30-06-23-07: Training (last 600 batches): accuracy = 0.969028, f1-score = 0.970639, loss = 54.826461
2023-10-30-06-23-12: Validation (total 179 batches): accuracy = 0.958411, f1-score = 0.960948, loss = 23.703959
2023-10-30-06-23-17: Testing (total 179 batches): accuracy = 0.962617, f1-score = 0.965004, loss = 18.731394
2023-10-30-06-23-17: Finished batch 1200.

2023-10-30-06-24-03: Training (last 600 batches): accuracy = 0.983889, f1-score = 0.984701, loss = 31.263843
2023-10-30-06-24-08: Validation (total 179 batches): accuracy = 0.960748, f1-score = 0.962128, loss = 22.090403
2023-10-30-06-24-13: Testing (total 179 batches): accuracy = 0.961215, f1-score = 0.962764, loss = 17.987020
2023-10-30-06-24-13: Finished batch 1800.

2023-10-30-06-24-59: Training (last 600 batches): accuracy = 0.991111, f1-score = 0.991590, loss = 19.048429
2023-10-30-06-25-04: Validation (total 179 batches): accuracy = 0.964019, f1-score = 0.965640, loss = 22.200396
2023-10-30-06-25-09: Testing (total 179 batches): accuracy = 0.964953, f1-score = 0.966563, loss = 18.898325
2023-10-30-06-25-09: Finished batch 2400.

2023-10-30-06-25-55: Training (last 600 batches): accuracy = 0.991944, f1-score = 0.992219, loss = 15.323792
2023-10-30-06-26-00: Validation (total 179 batches): accuracy = 0.944860, f1-score = 0.945622, loss = 37.044872
2023-10-30-06-26-05: Testing (total 179 batches): accuracy = 0.953271, f1-score = 0.953789, loss = 27.139790
2023-10-30-06-26-05: Finished batch 3000.

2023-10-30-06-26-51: Training (last 600 batches): accuracy = 0.992639, f1-score = 0.992994, loss = 12.676061
2023-10-30-06-26-56: Validation (total 179 batches): accuracy = 0.958879, f1-score = 0.960432, loss = 28.980192
2023-10-30-06-27-01: Testing (total 179 batches): accuracy = 0.961682, f1-score = 0.963063, loss = 23.431223
2023-10-30-06-27-01: Finished batch 3600.

2023-10-30-06-27-47: Training (last 600 batches): accuracy = 0.995278, f1-score = 0.995457, loss = 9.321779
2023-10-30-06-27-52: Validation (total 179 batches): accuracy = 0.966355, f1-score = 0.968142, loss = 28.568792
2023-10-30-06-27-57: Testing (total 179 batches): accuracy = 0.969159, f1-score = 0.970874, loss = 25.297447
2023-10-30-06-27-57: Finished batch 4200.

2023-10-30-06-28-43: Training (last 600 batches): accuracy = 0.997917, f1-score = 0.998012, loss = 4.208148
2023-10-30-06-28-48: Validation (total 179 batches): accuracy = 0.960748, f1-score = 0.962264, loss = 24.914288
2023-10-30-06-28-53: Testing (total 179 batches): accuracy = 0.958411, f1-score = 0.959747, loss = 23.732132
2023-10-30-06-28-53: Finished batch 4800.

2023-10-30-06-29-35: Training (last 600 batches): accuracy = 0.992361, f1-score = 0.992681, loss = 13.935195
2023-10-30-06-29-40: Validation (total 179 batches): accuracy = 0.968224, f1-score = 0.969616, loss = 25.920279
2023-10-30-06-29-45: Testing (total 179 batches): accuracy = 0.968224, f1-score = 0.969751, loss = 22.423845
2023-10-30-06-29-45: Finished batch 5350.

