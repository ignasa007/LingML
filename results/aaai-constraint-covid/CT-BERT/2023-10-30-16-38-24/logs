DATASET = aaai-constraint-covid
COVID WORDS = covid 19, covid-19, covid19, covid, corona virus, coronavirus, corona
TOKENS = [URL], [HASHTAG], [MENTION], [EMOJI], [COVID]
ADD NEW TOKENS = False
MAX LENGTH = 128
BATCH SIZE = 12
MODEL = covid-twitter-bert-v2
LEARNING RATE = 1e-05
TEST EVERY = 600
SAVE EVERY = None

2023-10-30-16-38-24: Loading and pre-processing datasets...
2023-10-30-16-38-26: Finished pre-processing datasets.

2023-10-30-16-38-26: Tokenizing datasets...
2023-10-30-16-38-28: Finished tokenizing datasets.

2023-10-30-16-38-28: Preparing data-loaders...
2023-10-30-16-38-28: Finished preparing data-loaders.

2023-10-30-16-38-28: Loading and preparing model...
2023-10-30-16-38-32: Finshed preparing model.

2023-10-30-16-38-32: Starting training...

2023-10-30-16-40-55: Training (last 600 batches): accuracy = 0.930000, f1-score = 0.933351, loss = 117.039831
2023-10-30-16-41-08: Validation (total 179 batches): accuracy = 0.974299, f1-score = 0.975479, loss = 14.107219
2023-10-30-16-41-20: Testing (total 179 batches): accuracy = 0.977103, f1-score = 0.978096, loss = 14.469726
2023-10-30-16-41-20: Finished batch 600.

2023-10-30-16-43-42: Training (last 600 batches): accuracy = 0.984444, f1-score = 0.985091, loss = 34.190770
2023-10-30-16-43-55: Validation (total 179 batches): accuracy = 0.974766, f1-score = 0.975676, loss = 15.863685
2023-10-30-16-44-08: Testing (total 179 batches): accuracy = 0.976168, f1-score = 0.977058, loss = 14.938090
2023-10-30-16-44-08: Finished batch 1200.

2023-10-30-16-46-28: Training (last 600 batches): accuracy = 0.991667, f1-score = 0.992080, loss = 17.099429
2023-10-30-16-46-40: Validation (total 179 batches): accuracy = 0.977103, f1-score = 0.978271, loss = 14.587236
2023-10-30-16-46-53: Testing (total 179 batches): accuracy = 0.976168, f1-score = 0.977364, loss = 15.957511
2023-10-30-16-46-53: Finished batch 1800.

2023-10-30-16-49-13: Training (last 600 batches): accuracy = 0.995278, f1-score = 0.995444, loss = 8.615645
2023-10-30-16-49-26: Validation (total 179 batches): accuracy = 0.980374, f1-score = 0.981283, loss = 17.835133
2023-10-30-16-49-39: Testing (total 179 batches): accuracy = 0.979439, f1-score = 0.980427, loss = 17.428509
2023-10-30-16-49-39: Finished batch 2400.

2023-10-30-16-52-00: Training (last 600 batches): accuracy = 0.994861, f1-score = 0.995139, loss = 11.947927
2023-10-30-16-52-12: Validation (total 179 batches): accuracy = 0.975234, f1-score = 0.976371, loss = 16.846992
2023-10-30-16-52-25: Testing (total 179 batches): accuracy = 0.978037, f1-score = 0.979046, loss = 15.753265
2023-10-30-16-52-25: Finished batch 3000.

2023-10-30-16-54-46: Training (last 600 batches): accuracy = 0.997083, f1-score = 0.997206, loss = 5.618779
2023-10-30-16-54-59: Validation (total 179 batches): accuracy = 0.980841, f1-score = 0.981850, loss = 16.312981
2023-10-30-16-55-12: Testing (total 179 batches): accuracy = 0.977570, f1-score = 0.978761, loss = 17.337259
2023-10-30-16-55-12: Finished batch 3600.

2023-10-30-16-57-33: Training (last 600 batches): accuracy = 0.999861, f1-score = 0.999868, loss = 0.791014
2023-10-30-16-57-46: Validation (total 179 batches): accuracy = 0.978972, f1-score = 0.980097, loss = 21.318050
2023-10-30-16-57-59: Testing (total 179 batches): accuracy = 0.978972, f1-score = 0.980027, loss = 20.686842
2023-10-30-16-57-59: Finished batch 4200.

2023-10-30-17-00-20: Training (last 600 batches): accuracy = 0.998056, f1-score = 0.998141, loss = 5.607762
2023-10-30-17-00-33: Validation (total 179 batches): accuracy = 0.975701, f1-score = 0.976991, loss = 17.476404
2023-10-30-17-00-46: Testing (total 179 batches): accuracy = 0.973832, f1-score = 0.975155, loss = 18.390821
2023-10-30-17-00-46: Finished batch 4800.

2023-10-30-17-02-56: Training (last 600 batches): accuracy = 0.995694, f1-score = 0.995868, loss = 8.507431
2023-10-30-17-03-08: Validation (total 179 batches): accuracy = 0.974299, f1-score = 0.975566, loss = 23.943897
2023-10-30-17-03-21: Testing (total 179 batches): accuracy = 0.977570, f1-score = 0.978591, loss = 20.990177
2023-10-30-17-03-21: Finished batch 5350.

