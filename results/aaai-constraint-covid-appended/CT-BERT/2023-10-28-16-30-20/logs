DATASET = aaai-constraint-covid-appended
COVID WORDS = covid 19, covid-19, covid19, covid, corona virus, coronavirus, corona
TOKENS = [URL], [HASHTAG], [MENTION], [EMOJI], [COVID]
ADD NEW TOKENS = False
MAX LENGTH = 128
BATCH SIZE = 12
MODEL = covid-twitter-bert-v2
LEARNING RATE = 1e-05
TEST EVERY = 600
SAVE EVERY = None

2023-10-28-16-30-20: Loading and pre-processing datasets...
2023-10-28-16-30-22: Finished pre-processing datasets.

2023-10-28-16-30-22: Tokenizing datasets...
2023-10-28-16-30-25: Finished tokenizing datasets.

2023-10-28-16-30-25: Preparing data-loaders...
2023-10-28-16-30-25: Finished preparing data-loaders.

2023-10-28-16-30-25: Loading and preparing model...
2023-10-28-16-30-28: Finshed preparing model.

2023-10-28-16-30-28: Starting training...

2023-10-28-16-32-52: Training (last 600 batches): accuracy = 0.940833, f1-score = 0.944168, loss = 98.345177
2023-10-28-16-33-05: Validation (total 179 batches): accuracy = 0.974766, f1-score = 0.975850, loss = 15.213953
2023-10-28-16-33-18: Testing (total 179 batches): accuracy = 0.979439, f1-score = 0.980269, loss = 13.139176
2023-10-28-16-33-18: Finished batch 600.

2023-10-28-16-35-40: Training (last 600 batches): accuracy = 0.984444, f1-score = 0.985244, loss = 31.425307
2023-10-28-16-35-53: Validation (total 179 batches): accuracy = 0.976636, f1-score = 0.977817, loss = 13.303201
2023-10-28-16-36-06: Testing (total 179 batches): accuracy = 0.980374, f1-score = 0.981317, loss = 12.364192
2023-10-28-16-36-06: Finished batch 1200.

2023-10-28-16-38-26: Training (last 600 batches): accuracy = 0.993611, f1-score = 0.993920, loss = 13.580418
2023-10-28-16-38-39: Validation (total 179 batches): accuracy = 0.982243, f1-score = 0.983201, loss = 13.983211
2023-10-28-16-38-52: Testing (total 179 batches): accuracy = 0.981308, f1-score = 0.982206, loss = 12.438509
2023-10-28-16-38-52: Finished batch 1800.

2023-10-28-16-41-12: Training (last 600 batches): accuracy = 0.996667, f1-score = 0.996779, loss = 7.512223
2023-10-28-16-41-24: Validation (total 179 batches): accuracy = 0.972897, f1-score = 0.973756, loss = 16.787027
2023-10-28-16-41-37: Testing (total 179 batches): accuracy = 0.974299, f1-score = 0.975057, loss = 15.670187
2023-10-28-16-41-37: Finished batch 2400.

2023-10-28-16-43-57: Training (last 600 batches): accuracy = 0.995417, f1-score = 0.995632, loss = 8.403886
2023-10-28-16-44-10: Validation (total 179 batches): accuracy = 0.975701, f1-score = 0.977072, loss = 18.129692
2023-10-28-16-44-23: Testing (total 179 batches): accuracy = 0.978972, f1-score = 0.980097, loss = 14.839422
2023-10-28-16-44-23: Finished batch 3000.

2023-10-28-16-46-43: Training (last 600 batches): accuracy = 0.997639, f1-score = 0.997761, loss = 5.246039
2023-10-28-16-46-55: Validation (total 179 batches): accuracy = 0.980374, f1-score = 0.981399, loss = 16.912466
2023-10-28-16-47-08: Testing (total 179 batches): accuracy = 0.979439, f1-score = 0.980514, loss = 17.342838
2023-10-28-16-47-08: Finished batch 3600.

2023-10-28-16-49-28: Training (last 600 batches): accuracy = 0.997361, f1-score = 0.997454, loss = 5.707141
2023-10-28-16-49-41: Validation (total 179 batches): accuracy = 0.972430, f1-score = 0.973531, loss = 16.073013
2023-10-28-16-49-54: Testing (total 179 batches): accuracy = 0.973832, f1-score = 0.974910, loss = 15.846497
2023-10-28-16-49-54: Finished batch 4200.

2023-10-28-16-52-14: Training (last 600 batches): accuracy = 0.997639, f1-score = 0.997749, loss = 5.686442
2023-10-28-16-52-26: Validation (total 179 batches): accuracy = 0.971028, f1-score = 0.972926, loss = 22.540794
2023-10-28-16-52-39: Testing (total 179 batches): accuracy = 0.971495, f1-score = 0.973351, loss = 21.686775
2023-10-28-16-52-39: Finished batch 4800.

2023-10-28-16-54-47: Training (last 600 batches): accuracy = 0.999444, f1-score = 0.999470, loss = 1.215732
2023-10-28-16-55-00: Validation (total 179 batches): accuracy = 0.980841, f1-score = 0.981802, loss = 19.366222
2023-10-28-16-55-13: Testing (total 179 batches): accuracy = 0.979439, f1-score = 0.980392, loss = 19.848846
2023-10-28-16-55-13: Finished batch 5350.

