DATASET = aaai-constraint-covid-appended
COVID WORDS = covid 19, covid-19, covid19, covid, corona virus, coronavirus, corona
TOKENS = [URL], [HASHTAG], [MENTION], [EMOJI], [COVID]
ADD NEW TOKENS = False
MAX LENGTH = 128
BATCH SIZE = 12
MODEL = covid-twitter-bert-v2
LEARNING RATE = 1e-05
TEST EVERY = 600
SAVE EVERY = None

2023-10-30-14-59-14: Loading and pre-processing datasets...
2023-10-30-14-59-16: Finished pre-processing datasets.

2023-10-30-14-59-16: Tokenizing datasets...
2023-10-30-14-59-18: Finished tokenizing datasets.

2023-10-30-14-59-18: Preparing data-loaders...
2023-10-30-14-59-18: Finished preparing data-loaders.

2023-10-30-14-59-18: Loading and preparing model...
2023-10-30-14-59-22: Finshed preparing model.

2023-10-30-14-59-22: Starting training...

2023-10-30-15-01-49: Training (last 600 batches): accuracy = 0.902917, f1-score = 0.909934, loss = 154.202806
2023-10-30-15-02-01: Validation (total 179 batches): accuracy = 0.969159, f1-score = 0.971002, loss = 18.354877
2023-10-30-15-02-14: Testing (total 179 batches): accuracy = 0.976636, f1-score = 0.977876, loss = 15.374510
2023-10-30-15-02-14: Finished batch 600.

2023-10-30-15-04-39: Training (last 600 batches): accuracy = 0.981111, f1-score = 0.981987, loss = 37.338008
2023-10-30-15-04-52: Validation (total 179 batches): accuracy = 0.976636, f1-score = 0.977619, loss = 12.560397
2023-10-30-15-05-05: Testing (total 179 batches): accuracy = 0.982710, f1-score = 0.983460, loss = 11.853519
2023-10-30-15-05-05: Finished batch 1200.

2023-10-30-15-07-28: Training (last 600 batches): accuracy = 0.990000, f1-score = 0.990496, loss = 21.507141
2023-10-30-15-07-41: Validation (total 179 batches): accuracy = 0.978505, f1-score = 0.979700, loss = 14.808899
2023-10-30-15-07-53: Testing (total 179 batches): accuracy = 0.978972, f1-score = 0.980097, loss = 14.476747
2023-10-30-15-07-53: Finished batch 1800.

2023-10-30-15-10-15: Training (last 600 batches): accuracy = 0.995278, f1-score = 0.995500, loss = 9.121041
2023-10-30-15-10-28: Validation (total 179 batches): accuracy = 0.982243, f1-score = 0.982975, loss = 13.759715
2023-10-30-15-10-40: Testing (total 179 batches): accuracy = 0.983178, f1-score = 0.983957, loss = 14.163713
2023-10-30-15-10-40: Finished batch 2400.

2023-10-30-15-13-01: Training (last 600 batches): accuracy = 0.995000, f1-score = 0.995177, loss = 8.688587
2023-10-30-15-13-14: Validation (total 179 batches): accuracy = 0.978037, f1-score = 0.979304, loss = 16.737490
2023-10-30-15-13-27: Testing (total 179 batches): accuracy = 0.976168, f1-score = 0.977503, loss = 18.944143
2023-10-30-15-13-27: Finished batch 3000.

2023-10-30-15-15-47: Training (last 600 batches): accuracy = 0.998472, f1-score = 0.998559, loss = 2.982828
2023-10-30-15-16-00: Validation (total 179 batches): accuracy = 0.981776, f1-score = 0.982582, loss = 16.156689
2023-10-30-15-16-12: Testing (total 179 batches): accuracy = 0.982243, f1-score = 0.983081, loss = 16.766462
2023-10-30-15-16-12: Finished batch 3600.

2023-10-30-15-18-32: Training (last 600 batches): accuracy = 0.996250, f1-score = 0.996395, loss = 6.962966
2023-10-30-15-18-45: Validation (total 179 batches): accuracy = 0.971495, f1-score = 0.972584, loss = 16.849125
2023-10-30-15-18-58: Testing (total 179 batches): accuracy = 0.975234, f1-score = 0.976201, loss = 16.748949
2023-10-30-15-18-58: Finished batch 4200.

2023-10-30-15-21-17: Training (last 600 batches): accuracy = 0.998194, f1-score = 0.998279, loss = 3.339265
2023-10-30-15-21-30: Validation (total 179 batches): accuracy = 0.980374, f1-score = 0.981399, loss = 19.642218
2023-10-30-15-21-43: Testing (total 179 batches): accuracy = 0.978037, f1-score = 0.979139, loss = 20.034302
2023-10-30-15-21-43: Finished batch 4800.

2023-10-30-15-23-51: Training (last 600 batches): accuracy = 0.999861, f1-score = 0.999866, loss = 0.443363
2023-10-30-15-24-04: Validation (total 179 batches): accuracy = 0.978972, f1-score = 0.980097, loss = 22.085667
2023-10-30-15-24-16: Testing (total 179 batches): accuracy = 0.979439, f1-score = 0.980496, loss = 21.146057
2023-10-30-15-24-16: Finished batch 5350.

