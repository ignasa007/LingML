DATASET = aaai-constraint-covid-cleaned
COVID WORDS = covid 19, covid-19, covid19, covid, corona virus, coronavirus, corona
TOKENS = [URL], [HASHTAG], [MENTION], [EMOJI], [COVID]
ADD NEW TOKENS = True
MAX LENGTH = 128
BATCH SIZE = 12
MODEL = covid-twitter-bert-v2
LEARNING RATE = 1e-05
TEST EVERY = 300
SAVE EVERY = None

2023-08-09-17-26-50: Loading and pre-processing datasets...
2023-08-09-17-26-51: Finished pre-processing datasets.

2023-08-09-17-26-51: Tokenizing datasets...
2023-08-09-17-26-52: Finished tokenizing datasets.

2023-08-09-17-26-52: Preparing data-loaders...
2023-08-09-17-26-52: Finished preparing data-loaders.

2023-08-09-17-26-52: Loading and preparing model...
2023-08-09-17-26-55: Finshed preparing model.

2023-08-09-17-26-55: Starting training...

2023-08-09-17-28-00: Training (last 300 batches): accuracy = 0.913056, f1-score = 0.931495, loss = 71.543180
2023-08-09-17-28-05: Validation (total 82 batches): accuracy = 0.950000, f1-score = 0.960324, loss = 11.612730
2023-08-09-17-28-05: Finished batch 300.

2023-08-09-17-29-09: Training (last 300 batches): accuracy = 0.972740, f1-score = 0.978319, loss = 31.032870
2023-08-09-17-29-15: Validation (total 82 batches): accuracy = 0.952041, f1-score = 0.962005, loss = 12.537392
2023-08-09-17-29-15: Finished batch 600.

2023-08-09-17-30-19: Training (last 300 batches): accuracy = 0.981085, f1-score = 0.984976, loss = 19.250186
2023-08-09-17-30-24: Validation (total 82 batches): accuracy = 0.956122, f1-score = 0.965295, loss = 12.915454
2023-08-09-17-30-24: Finished batch 900.

2023-08-09-17-31-28: Training (last 300 batches): accuracy = 0.989708, f1-score = 0.992017, loss = 12.403775
2023-08-09-17-31-34: Validation (total 82 batches): accuracy = 0.956122, f1-score = 0.965572, loss = 13.822302
2023-08-09-17-31-34: Finished batch 1200.

2023-08-09-17-32-38: Training (last 300 batches): accuracy = 0.987761, f1-score = 0.990313, loss = 13.371564
2023-08-09-17-32-43: Validation (total 82 batches): accuracy = 0.951020, f1-score = 0.962025, loss = 17.751926
2023-08-09-17-32-43: Finished batch 1500.

2023-08-09-17-33-47: Training (last 300 batches): accuracy = 0.992211, f1-score = 0.993835, loss = 8.349960
2023-08-09-17-33-53: Validation (total 82 batches): accuracy = 0.956122, f1-score = 0.965351, loss = 15.866695
2023-08-09-17-33-53: Finished batch 1800.

2023-08-09-17-34-57: Training (last 300 batches): accuracy = 0.993046, f1-score = 0.994485, loss = 8.289613
2023-08-09-17-35-02: Validation (total 82 batches): accuracy = 0.956122, f1-score = 0.965295, loss = 15.013517
2023-08-09-17-35-02: Finished batch 2100.

2023-08-09-17-36-06: Training (last 300 batches): accuracy = 0.993880, f1-score = 0.995269, loss = 6.493506
2023-08-09-17-36-11: Validation (total 82 batches): accuracy = 0.943878, f1-score = 0.957065, loss = 18.645262
2023-08-09-17-36-11: Finished batch 2400.

2023-08-09-17-37-16: Training (last 300 batches): accuracy = 0.994159, f1-score = 0.995355, loss = 5.772866
2023-08-09-17-37-21: Validation (total 82 batches): accuracy = 0.956122, f1-score = 0.965462, loss = 16.079258
2023-08-09-17-37-21: Finished batch 2700.

2023-08-09-17-38-25: Training (last 300 batches): accuracy = 0.997218, f1-score = 0.997804, loss = 3.110987
2023-08-09-17-38-30: Validation (total 82 batches): accuracy = 0.955102, f1-score = 0.964459, loss = 16.725910
2023-08-09-17-38-30: Finished batch 3000.

2023-08-09-17-39-28: Training (last 300 batches): accuracy = 0.996106, f1-score = 0.996958, loss = 4.079062
2023-08-09-17-39-33: Validation (total 82 batches): accuracy = 0.946939, f1-score = 0.959055, loss = 21.564476
2023-08-09-17-39-33: Finished batch 3270.

