DATASET = aaai-constraint-covid-cleaned-appended
COVID WORDS = covid 19, covid-19, covid19, covid, corona virus, coronavirus, corona
TOKENS = [URL], [HASHTAG], [MENTION], [EMOJI], [COVID]
ADD NEW TOKENS = False
MAX LENGTH = 128
BATCH SIZE = 12
MODEL = covid-twitter-bert-v2
LEARNING RATE = 1e-05
TEST EVERY = 300
SAVE EVERY = None

2023-08-10-01-38-20: Loading and pre-processing datasets...
2023-08-10-01-38-20: Finished pre-processing datasets.

2023-08-10-01-38-20: Tokenizing datasets...
2023-08-10-01-38-22: Finished tokenizing datasets.

2023-08-10-01-38-22: Preparing data-loaders...
2023-08-10-01-38-22: Finished preparing data-loaders.

2023-08-10-01-38-22: Loading and preparing model...
2023-08-10-01-38-25: Finshed preparing model.

2023-08-10-01-38-25: Starting training...

2023-08-10-01-39-30: Training (last 300 batches): accuracy = 0.912500, f1-score = 0.933921, loss = 73.905533
2023-08-10-01-39-35: Validation (total 82 batches): accuracy = 0.962245, f1-score = 0.970185, loss = 9.917292
2023-08-10-01-39-35: Finished batch 300.

2023-08-10-01-40-40: Training (last 300 batches): accuracy = 0.967455, f1-score = 0.974426, loss = 34.945346
2023-08-10-01-40-45: Validation (total 82 batches): accuracy = 0.958163, f1-score = 0.967121, loss = 9.995231
2023-08-10-01-40-45: Finished batch 600.

2023-08-10-01-41-50: Training (last 300 batches): accuracy = 0.983588, f1-score = 0.987042, loss = 20.122206
2023-08-10-01-41-55: Validation (total 82 batches): accuracy = 0.968367, f1-score = 0.974694, loss = 8.670349
2023-08-10-01-41-55: Finished batch 900.

2023-08-10-01-43-00: Training (last 300 batches): accuracy = 0.987483, f1-score = 0.990160, loss = 14.994155
2023-08-10-01-43-05: Validation (total 82 batches): accuracy = 0.961224, f1-score = 0.968801, loss = 9.429187
2023-08-10-01-43-05: Finished batch 1200.

2023-08-10-01-44-09: Training (last 300 batches): accuracy = 0.982754, f1-score = 0.986580, loss = 17.106020
2023-08-10-01-44-15: Validation (total 82 batches): accuracy = 0.966327, f1-score = 0.973279, loss = 8.719855
2023-08-10-01-44-15: Finished batch 1500.

2023-08-10-01-45-19: Training (last 300 batches): accuracy = 0.995549, f1-score = 0.996522, loss = 5.825732
2023-08-10-01-45-25: Validation (total 82 batches): accuracy = 0.968367, f1-score = 0.974899, loss = 9.894387
2023-08-10-01-45-25: Finished batch 1800.

2023-08-10-01-46-29: Training (last 300 batches): accuracy = 0.993602, f1-score = 0.995018, loss = 7.561550
2023-08-10-01-46-34: Validation (total 82 batches): accuracy = 0.966327, f1-score = 0.973409, loss = 11.008156
2023-08-10-01-46-34: Finished batch 2100.

2023-08-10-01-47-39: Training (last 300 batches): accuracy = 0.994437, f1-score = 0.995542, loss = 5.752812
2023-08-10-01-47-44: Validation (total 82 batches): accuracy = 0.970408, f1-score = 0.976480, loss = 10.872143
2023-08-10-01-47-44: Finished batch 2400.

2023-08-10-01-48-49: Training (last 300 batches): accuracy = 0.989430, f1-score = 0.991842, loss = 10.610103
2023-08-10-01-48-54: Validation (total 82 batches): accuracy = 0.954082, f1-score = 0.964029, loss = 14.005787
2023-08-10-01-48-54: Finished batch 2700.

2023-08-10-01-49-58: Training (last 300 batches): accuracy = 0.995271, f1-score = 0.996289, loss = 5.642723
2023-08-10-01-50-04: Validation (total 82 batches): accuracy = 0.964286, f1-score = 0.971614, loss = 12.123658
2023-08-10-01-50-04: Finished batch 3000.

2023-08-10-01-51-02: Training (last 300 batches): accuracy = 0.995828, f1-score = 0.996723, loss = 4.210152
2023-08-10-01-51-07: Validation (total 82 batches): accuracy = 0.967347, f1-score = 0.974026, loss = 11.792597
2023-08-10-01-51-07: Finished batch 3270.

