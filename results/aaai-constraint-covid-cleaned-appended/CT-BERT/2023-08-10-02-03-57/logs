DATASET = aaai-constraint-covid-cleaned-appended
COVID WORDS = covid 19, covid-19, covid19, covid, corona virus, coronavirus, corona
TOKENS = [URL], [HASHTAG], [MENTION], [EMOJI], [COVID]
ADD NEW TOKENS = False
MAX LENGTH = 128
BATCH SIZE = 12
MODEL = covid-twitter-bert-v2
LEARNING RATE = 1e-05
TEST EVERY = 300
SAVE EVERY = None

2023-08-10-02-03-57: Loading and pre-processing datasets...
2023-08-10-02-03-58: Finished pre-processing datasets.

2023-08-10-02-03-58: Tokenizing datasets...
2023-08-10-02-03-59: Finished tokenizing datasets.

2023-08-10-02-03-59: Preparing data-loaders...
2023-08-10-02-03-59: Finished preparing data-loaders.

2023-08-10-02-03-59: Loading and preparing model...
2023-08-10-02-04-03: Finshed preparing model.

2023-08-10-02-04-03: Starting training...

2023-08-10-02-05-08: Training (last 300 batches): accuracy = 0.899722, f1-score = 0.922148, loss = 84.874475
2023-08-10-02-05-13: Validation (total 82 batches): accuracy = 0.957143, f1-score = 0.965461, loss = 12.099206
2023-08-10-02-05-13: Finished batch 300.

2023-08-10-02-06-17: Training (last 300 batches): accuracy = 0.968011, f1-score = 0.975178, loss = 32.940298
2023-08-10-02-06-23: Validation (total 82 batches): accuracy = 0.966327, f1-score = 0.972569, loss = 10.265064
2023-08-10-02-06-23: Finished batch 600.

2023-08-10-02-07-27: Training (last 300 batches): accuracy = 0.978860, f1-score = 0.983536, loss = 23.449912
2023-08-10-02-07-32: Validation (total 82 batches): accuracy = 0.967347, f1-score = 0.973597, loss = 10.651258
2023-08-10-02-07-32: Finished batch 900.

2023-08-10-02-08-37: Training (last 300 batches): accuracy = 0.986648, f1-score = 0.989660, loss = 15.938134
2023-08-10-02-08-42: Validation (total 82 batches): accuracy = 0.970408, f1-score = 0.975894, loss = 10.941208
2023-08-10-02-08-42: Finished batch 1200.

2023-08-10-02-09-47: Training (last 300 batches): accuracy = 0.987761, f1-score = 0.990261, loss = 12.801906
2023-08-10-02-09-52: Validation (total 82 batches): accuracy = 0.961224, f1-score = 0.968750, loss = 12.092232
2023-08-10-02-09-52: Finished batch 1500.

2023-08-10-02-10-56: Training (last 300 batches): accuracy = 0.994993, f1-score = 0.996121, loss = 7.267445
2023-08-10-02-11-02: Validation (total 82 batches): accuracy = 0.965306, f1-score = 0.971901, loss = 12.473178
2023-08-10-02-11-02: Finished batch 1800.

2023-08-10-02-12-06: Training (last 300 batches): accuracy = 0.995549, f1-score = 0.996471, loss = 5.262061
2023-08-10-02-12-11: Validation (total 82 batches): accuracy = 0.957143, f1-score = 0.965517, loss = 14.964465
2023-08-10-02-12-11: Finished batch 2100.

2023-08-10-02-13-16: Training (last 300 batches): accuracy = 0.995549, f1-score = 0.996559, loss = 4.581736
2023-08-10-02-13-21: Validation (total 82 batches): accuracy = 0.955102, f1-score = 0.964111, loss = 18.090240
2023-08-10-02-13-21: Finished batch 2400.

2023-08-10-02-14-25: Training (last 300 batches): accuracy = 0.992490, f1-score = 0.994101, loss = 7.347763
2023-08-10-02-14-31: Validation (total 82 batches): accuracy = 0.946939, f1-score = 0.957724, loss = 18.170006
2023-08-10-02-14-31: Finished batch 2700.

2023-08-10-02-15-35: Training (last 300 batches): accuracy = 0.996384, f1-score = 0.997192, loss = 3.861018
2023-08-10-02-15-40: Validation (total 82 batches): accuracy = 0.963265, f1-score = 0.970492, loss = 16.518187
2023-08-10-02-15-40: Finished batch 3000.

2023-08-10-02-16-38: Training (last 300 batches): accuracy = 0.995549, f1-score = 0.996514, loss = 4.155501
2023-08-10-02-16-44: Validation (total 82 batches): accuracy = 0.960204, f1-score = 0.968059, loss = 15.730798
2023-08-10-02-16-44: Finished batch 3270.

