{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mlp\n",
    "\n",
    "FONTDICT = {\n",
    "    'family': 'serif', \n",
    "    'color': 'black', \n",
    "    'weight': 'normal', \n",
    "    'size': 12\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(dataset, models):\n",
    "\n",
    "    valid_models = set(models)\n",
    "    results = defaultdict(list)\n",
    "\n",
    "    for model in valid_models.copy():\n",
    "\n",
    "        exp_dir = f'./results/{dataset}/{model}'\n",
    "        for run in os.listdir(exp_dir):\n",
    "            run_dir = f'{exp_dir}/{run}'\n",
    "            if os.path.isdir(run_dir):\n",
    "                results[model].append(dict())\n",
    "                for split in ('training', 'validation', 'testing'):\n",
    "                    fn = f'{run_dir}/{split}_results.pkl'\n",
    "                    if os.path.isfile(fn):\n",
    "                        with open(fn, 'rb') as f:\n",
    "                            results[model][-1][split] = dict(pickle.load(f))\n",
    "                if not results[model][-1]:\n",
    "                    results[model].pop()\n",
    "    \n",
    "    return dict(results), tuple(valid_models)\n",
    "\n",
    "def loss(data, idxs, jump):\n",
    "    ys = list()\n",
    "    for idx in idxs:\n",
    "        loss = sum(data['loss'][idx-jump:idx])\n",
    "        tp, tn, fp, fn = (\n",
    "            sum(data['tp'][idx-jump:idx]),\n",
    "            sum(data['tn'][idx-jump:idx]),\n",
    "            sum(data['fp'][idx-jump:idx]),\n",
    "            sum(data['fn'][idx-jump:idx]),\n",
    "        )\n",
    "        ys.append(loss / (tp+tn+fp+fn))\n",
    "    return ys\n",
    "\n",
    "def accuracy(data, idxs, jump):\n",
    "    ys = list()\n",
    "    for idx in idxs:\n",
    "        tp, tn, fp, fn = (\n",
    "            sum(data['tp'][idx-jump:idx]),\n",
    "            sum(data['tn'][idx-jump:idx]),\n",
    "            sum(data['fp'][idx-jump:idx]),\n",
    "            sum(data['fn'][idx-jump:idx]),\n",
    "        )\n",
    "        acc = (tp+tn) / (tp+tn+fp+fn)\n",
    "        ys.append(acc)\n",
    "    return ys\n",
    "\n",
    "def f1_score(data, idxs, jump):\n",
    "    ys = list()\n",
    "    for idx in idxs:\n",
    "        tp, fp, fn = (\n",
    "            sum(data['tp'][idx-jump:idx]),\n",
    "            sum(data['fp'][idx-jump:idx]),\n",
    "            sum(data['fn'][idx-jump:idx]),\n",
    "        )\n",
    "        f1 = (tp+tp) / (tp+tp+fp+fn)\n",
    "        ys.append(f1)\n",
    "    return ys\n",
    "\n",
    "fn_map = {\n",
    "    'Loss': loss,\n",
    "    'Accuracy': accuracy,\n",
    "    'F1-Score': f1_score\n",
    "}\n",
    "\n",
    "models = ['ALBERT', 'BERT', 'BERTweet', 'CT-BERT', 'DistilBERT', 'Longformer', 'RoBERTa', 'Twitter-RoBERTa', 'XLM', 'XLM-RoBERTa', 'XLNet',]\n",
    "pairs = (\n",
    "    ('CT-BERT', 'Twitter-RoBERTa'), \n",
    "    ('CT-BERT-NT', 'Twitter-RoBERTa-NT'), \n",
    "    ('CT-BERT', 'CT-BERT-NT'), \n",
    "    ('Twitter-RoBERTa', 'Twitter-RoBERTa-NT'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at vinai/bertweet-covid19-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-covid19-base-uncased and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at digitalepidemiologylab/covid-twitter-bert-v2 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at digitalepidemiologylab/covid-twitter-bert-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at xlm-mlm-en-2048 were not used when initializing XLMForSequenceClassification: ['pred_layer.proj.weight', 'pred_layer.proj.bias']\n",
      "- This IS expected if you are initializing XLMForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMForSequenceClassification were not initialized from the model checkpoint at xlm-mlm-en-2048 and are newly initialized: ['sequence_summary.summary.bias', 'sequence_summary.summary.weight', 'transformer.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight', 'logits_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "from model_classes import modelclass_map\n",
    "from data_classes.preprocess import PreProcessor\n",
    "\n",
    "preprocessor = PreProcessor()\n",
    "parameters = dict()\n",
    "\n",
    "for model_name in os.listdir('config/models'):\n",
    "    if model_name.startswith('bart'): continue\n",
    "    with open(f'config/models/{model_name}', 'r') as f:\n",
    "        hf_path = f.readline().split(\"'\")[1]\n",
    "        f.readline(); save_name = f.readline().split(\"'\")[1]\n",
    "        tokenizer = AutoTokenizer.from_pretrained(hf_path)\n",
    "        tokenizer.add_tokens(preprocessor.TOKENS)\n",
    "        base_model = AutoModelForSequenceClassification.from_pretrained(hf_path)\n",
    "        model = modelclass_map(model_name.rstrip('.yaml'))(\n",
    "            base_model=base_model,\n",
    "            emb_table_size=len(tokenizer),\n",
    "            dense_size=18\n",
    "        )\n",
    "        parameters[save_name] = model.n_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALBERT           11.686M\n",
      "BERT            109.488M\n",
      "BERTweet        134.905M\n",
      "CT-BERT         335.149M\n",
      "DistilBERT       66.959M\n",
      "Longformer      148.665M\n",
      "RoBERTa         124.651M\n",
      "Twitter-RoBERTa 124.651M\n",
      "XLM             667.103M\n",
      "XLM-RoBERTa     278.049M\n",
      "XLNet           117.314M\n"
     ]
    }
   ],
   "source": [
    "for k, v in parameters.items():\n",
    "    print(k.ljust(15, ' '), f'{v[0]/10**6:.3f}M'.rjust(8, \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_062f4_row0_col0,#T_062f4_row0_col1,#T_062f4_row0_col2,#T_062f4_row1_col0,#T_062f4_row1_col1,#T_062f4_row1_col2,#T_062f4_row2_col0,#T_062f4_row2_col1,#T_062f4_row2_col2,#T_062f4_row3_col0,#T_062f4_row3_col1,#T_062f4_row3_col2,#T_062f4_row4_col0,#T_062f4_row4_col1,#T_062f4_row4_col2,#T_062f4_row5_col0,#T_062f4_row5_col1,#T_062f4_row5_col2,#T_062f4_row6_col0,#T_062f4_row6_col1,#T_062f4_row6_col2,#T_062f4_row7_col0,#T_062f4_row7_col1,#T_062f4_row7_col2,#T_062f4_row8_col0,#T_062f4_row8_col1,#T_062f4_row8_col2,#T_062f4_row9_col0,#T_062f4_row9_col1,#T_062f4_row9_col2,#T_062f4_row10_col0,#T_062f4_row10_col1,#T_062f4_row10_col2{\n",
       "            text-align:  right;\n",
       "        }</style><table id=\"T_062f4_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >acc</th>        <th class=\"col_heading level0 col1\" >acc-appended</th>        <th class=\"col_heading level0 col2\" >improvement</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_062f4_level0_row0\" class=\"row_heading level0 row0\" >ALBERT</th>\n",
       "                        <td id=\"T_062f4_row0_col0\" class=\"data row0 col0\" >3.374 \\pm 0.157</td>\n",
       "                        <td id=\"T_062f4_row0_col1\" class=\"data row0 col1\" >2.991 \\pm 0.107</td>\n",
       "                        <td id=\"T_062f4_row0_col2\" class=\"data row0 col2\" >11.357341</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_062f4_level0_row1\" class=\"row_heading level0 row1\" >BERT</th>\n",
       "                        <td id=\"T_062f4_row1_col0\" class=\"data row1 col0\" >3.215 \\pm 0.197</td>\n",
       "                        <td id=\"T_062f4_row1_col1\" class=\"data row1 col1\" >2.748 \\pm 0.100</td>\n",
       "                        <td id=\"T_062f4_row1_col2\" class=\"data row1 col2\" >14.534884</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_062f4_level0_row2\" class=\"row_heading level0 row2\" >BERTweet</th>\n",
       "                        <td id=\"T_062f4_row2_col0\" class=\"data row2 col0\" >2.869 \\pm 0.063</td>\n",
       "                        <td id=\"T_062f4_row2_col1\" class=\"data row2 col1\" >2.607 \\pm 0.130</td>\n",
       "                        <td id=\"T_062f4_row2_col2\" class=\"data row2 col2\" >9.120521</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_062f4_level0_row3\" class=\"row_heading level0 row3\" >CT-BERT</th>\n",
       "                        <td id=\"T_062f4_row3_col0\" class=\"data row3 col0\" >2.111 \\pm 0.128</td>\n",
       "                        <td id=\"T_062f4_row3_col1\" class=\"data row3 col1\" >1.815 \\pm 0.078</td>\n",
       "                        <td id=\"T_062f4_row3_col2\" class=\"data row3 col2\" >14.022140</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_062f4_level0_row4\" class=\"row_heading level0 row4\" >DistilBERT</th>\n",
       "                        <td id=\"T_062f4_row4_col0\" class=\"data row4 col0\" >3.084 \\pm 0.170</td>\n",
       "                        <td id=\"T_062f4_row4_col1\" class=\"data row4 col1\" >2.776 \\pm 0.063</td>\n",
       "                        <td id=\"T_062f4_row4_col2\" class=\"data row4 col2\" >10.000000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_062f4_level0_row5\" class=\"row_heading level0 row5\" >Longformer</th>\n",
       "                        <td id=\"T_062f4_row5_col0\" class=\"data row5 col0\" >3.402 \\pm 0.190</td>\n",
       "                        <td id=\"T_062f4_row5_col1\" class=\"data row5 col1\" >2.888 \\pm 0.080</td>\n",
       "                        <td id=\"T_062f4_row5_col2\" class=\"data row5 col2\" >15.109890</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_062f4_level0_row6\" class=\"row_heading level0 row6\" >RoBERTa</th>\n",
       "                        <td id=\"T_062f4_row6_col0\" class=\"data row6 col0\" >3.196 \\pm 0.224</td>\n",
       "                        <td id=\"T_062f4_row6_col1\" class=\"data row6 col1\" >2.804 \\pm 0.221</td>\n",
       "                        <td id=\"T_062f4_row6_col2\" class=\"data row6 col2\" >12.280702</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_062f4_level0_row7\" class=\"row_heading level0 row7\" >Twitter-RoBERTa</th>\n",
       "                        <td id=\"T_062f4_row7_col0\" class=\"data row7 col0\" >2.944 \\pm 0.042</td>\n",
       "                        <td id=\"T_062f4_row7_col1\" class=\"data row7 col1\" >2.720 \\pm 0.108</td>\n",
       "                        <td id=\"T_062f4_row7_col2\" class=\"data row7 col2\" >7.619048</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_062f4_level0_row8\" class=\"row_heading level0 row8\" >XLM</th>\n",
       "                        <td id=\"T_062f4_row8_col0\" class=\"data row8 col0\" >3.299 \\pm 0.308</td>\n",
       "                        <td id=\"T_062f4_row8_col1\" class=\"data row8 col1\" >2.710 \\pm 0.139</td>\n",
       "                        <td id=\"T_062f4_row8_col2\" class=\"data row8 col2\" >17.847025</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_062f4_level0_row9\" class=\"row_heading level0 row9\" >XLM-RoBERTa</th>\n",
       "                        <td id=\"T_062f4_row9_col0\" class=\"data row9 col0\" >2.869 \\pm 0.096</td>\n",
       "                        <td id=\"T_062f4_row9_col1\" class=\"data row9 col1\" >2.570 \\pm 0.122</td>\n",
       "                        <td id=\"T_062f4_row9_col2\" class=\"data row9 col2\" >10.423453</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_062f4_level0_row10\" class=\"row_heading level0 row10\" >XLNet</th>\n",
       "                        <td id=\"T_062f4_row10_col0\" class=\"data row10 col0\" >3.355 \\pm 0.095</td>\n",
       "                        <td id=\"T_062f4_row10_col1\" class=\"data row10 col1\" >3.093 \\pm 0.116</td>\n",
       "                        <td id=\"T_062f4_row10_col2\" class=\"data row10 col2\" >7.799443</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x245fc93bca0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = (\n",
    "    'aaai-constraint-covid',\n",
    "    'aaai-constraint-covid-appended',\n",
    ")\n",
    "\n",
    "index = sorted(models)\n",
    "metric = 'Accuracy'\n",
    "jump = 1\n",
    "\n",
    "# dataframe = defaultdict(lambda: defaultdict(list))\n",
    "dataframe = defaultdict(list)\n",
    "\n",
    "for dataset in datasets:\n",
    "    col_name = dataset.replace('aaai-constraint-covid', 'acc')\n",
    "    results, _ = get_results(dataset, index)\n",
    "    for model in index:\n",
    "        scores = list()\n",
    "        for run in results[model]:\n",
    "            val_data = run['validation']\n",
    "            idxs = list(range(jump, len(val_data['batch'])+jump, jump))\n",
    "            val_metrics = fn_map[metric](val_data, idxs, jump)\n",
    "            best_val_metric = max(val_metrics)\n",
    "            test_data = run['testing']\n",
    "            test_metrics = fn_map[metric](test_data, idxs, jump)\n",
    "            best_test_metric = 0.\n",
    "            for val_metric, test_metric in zip(val_metrics, test_metrics):\n",
    "                if val_metric == best_val_metric:\n",
    "                    best_test_metric = max(test_metric, best_test_metric)\n",
    "            scores.append(best_test_metric)\n",
    "        dataframe[col_name].append((np.mean(scores), np.std(scores)))\n",
    "        # dataframe[col_name][model] = scores\n",
    "        # dataframe[col_name].extend(scores)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    # dict({dataset: values for dataset, values in dataframe.items()}),\n",
    "    # index=[y for x in ([model, *('' for _ in range(4))] for model in index) for y in x]\n",
    "    dict(\n",
    "        **{\n",
    "            dataset: [f'{100*(1-mean):.3f} \\pm {100*std:.3f}' for (mean, std) in values]\n",
    "            for dataset, values in dataframe.items()\n",
    "        }, \n",
    "        improvement = [100*(y[0]-x[0])/(1-x[0]) for x, y in zip(dataframe['acc'], dataframe['acc-appended'])]\n",
    "    ),\n",
    "    index=index\n",
    ")\n",
    "\n",
    "df.style.set_properties(**{'text-align': 'right'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE MODEL_J BETTER THAN BASE MODEL_I, BUT\n",
      "\tMODEL_I WITH LINGUISTIC FEATURES BETTER THAN MODEL_J WITH LINGUISTIC FEATURES\n",
      "2. ALBERT > XLNet: \n",
      "\t-0.554% -> +3.438% \n",
      "\t10.04 times smaller model\n",
      "4. BERT > RoBERTa: \n",
      "\t-0.581% -> +2.041% \n",
      "\t1.14 times smaller model\n",
      "\n",
      "BASE MODEL_J BETTER THAN BASE MODEL_I, BUT\n",
      "\tMODEL_I WITH LINGUISTIC FEATURES BETTER THAN BASE MODEL_J\n",
      "1. ALBERT > BERT:                  -4.709%  -> +7.5%    with model size ratio 9.37\n",
      "2. ALBERT > DistilBERT:            -8.587%  -> +3.125%  with model size ratio 5.73\n",
      "4. ALBERT > RoBERTa:               -5.263%  -> +6.875%  with model size ratio 10.67\n",
      "5. ALBERT > XLM:                   -2.216%  -> +10.313% with model size ratio 57.09\n",
      "6. ALBERT > XLNet:                 -0.554%  -> +12.187% with model size ratio 10.04\n",
      "7. BERT > BERTweet:                -10.756% -> +4.422%  with model size ratio 1.23\n",
      "10. BERT > RoBERTa:                -0.581%  -> +16.327% with model size ratio 1.14\n",
      "11. BERT > Twitter-RoBERTa:        -8.43%   -> +7.143%  with model size ratio 1.14\n",
      "13. BERT > XLM-RoBERTa:            -10.756% -> +4.422%  with model size ratio 2.54\n",
      "15. DistilBERT > BERTweet:         -6.97%   -> +3.367%  with model size ratio 2.01\n",
      "16. RoBERTa > BERTweet:            -10.234% -> +2.333%  with model size ratio 1.08\n",
      "17. Twitter-RoBERTa > BERTweet:    -2.54%   -> +5.498%  with model size ratio 1.08\n",
      "21. DistilBERT > Twitter-RoBERTa:  -4.545%  -> +6.061%  with model size ratio 1.86\n",
      "23. DistilBERT > XLM-RoBERTa:      -6.97%   -> +3.367%  with model size ratio 4.15\n",
      "26. Longformer > XLM:              -3.022%  -> +14.239% with model size ratio 4.49\n",
      "28. RoBERTa > Twitter-RoBERTa:     -7.895%  -> +5.0%    with model size ratio 1.00\n",
      "30. RoBERTa > XLM-RoBERTa:         -10.234% -> +2.333%  with model size ratio 2.23\n",
      "31. XLNet > RoBERTa:               -4.735%  -> +3.323%  with model size ratio 1.06\n",
      "33. Twitter-RoBERTa > XLM-RoBERTa: -2.54%   -> +5.498%  with model size ratio 2.23\n",
      "35. XLNet > XLM:                   -1.671%  -> +6.647%  with model size ratio 5.69\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def perc_change(v1, v2):\n",
    "    return round(100 * (v1-v2) / v2, 3)\n",
    "\n",
    "print('base model_j better than base model_i, but\\n\\tmodel_i with linguistic features better than model_j with linguistic features'.upper())\n",
    "improvements = list()\n",
    "for i, j in combinations(range(len(index)), 2):\n",
    "    if (dataframe['acc'][i][0] < dataframe['acc'][j][0]) and (dataframe['acc-appended'][i][0] > dataframe['acc-appended'][j][0]):\n",
    "        improvements.append((i, j))\n",
    "    elif (dataframe['acc'][i][0] > dataframe['acc'][j][0]) and (dataframe['acc-appended'][i][0] < dataframe['acc-appended'][j][0]):\n",
    "        improvements.append((j, i))\n",
    "for k, (i, j) in enumerate(improvements, 1):\n",
    "    if parameters[index[i]][0] > parameters[index[j]][0]:\n",
    "        continue\n",
    "    print(f\"{k}. {index[i]} > {index[j]}:\",\n",
    "        f\"\\n\\t{perc_change(1-dataframe['acc'][j][0], 1-dataframe['acc'][i][0])}% ->\",\n",
    "        f\"+{perc_change(1-dataframe['acc-appended'][j][0], 1-dataframe['acc-appended'][i][0])}%\",\n",
    "        f\"\\n\\t{parameters[index[j]][0]/parameters[index[i]][0]:.2f} times smaller model\"\n",
    "    )\n",
    "\n",
    "print('\\nbase model_j better than base model_i, but\\n\\tmodel_i with linguistic features better than base model_j'.upper())\n",
    "improvements = list()\n",
    "for i, j in combinations(range(len(index)), 2):\n",
    "    if dataframe['acc'][i][0] < dataframe['acc'][j][0] < dataframe['acc-appended'][i][0]:\n",
    "        improvements.append((i, j))\n",
    "    elif dataframe['acc'][j][0] < dataframe['acc'][i][0] < dataframe['acc-appended'][j][0]:\n",
    "        improvements.append((j, i))\n",
    "for k, (i, j) in enumerate(improvements, 1):\n",
    "    if parameters[index[i]][0] > parameters[index[j]][0]:\n",
    "        continue\n",
    "    print(f\"{k}. {index[i]} > {index[j]}:\".ljust(34, \" \"),\n",
    "        f\"{perc_change(1-dataframe['acc'][j][0], 1-dataframe['acc'][i][0])}%\".ljust(8, \" \"),\n",
    "        f\"-> +{perc_change(1-dataframe['acc'][j][0], 1-dataframe['acc-appended'][i][0])}%\".ljust(11, \" \"),\n",
    "        f\"with model size ratio {parameters[index[j]][0]/parameters[index[i]][0]:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accs = list()\n",
    "\n",
    "for dataset in datasets:\n",
    "    results, _ = get_results(dataset, index)\n",
    "    test_accs.append(list())\n",
    "    for model in index:\n",
    "        scores = list()\n",
    "        for run in results[model]:\n",
    "            val_data = run['validation']\n",
    "            idxs = list(range(jump, len(val_data['batch'])+jump, jump))\n",
    "            val_metrics = fn_map[metric](val_data, idxs, jump)\n",
    "            best_val_metric = max(val_metrics)\n",
    "            test_data = run['testing']\n",
    "            test_metrics = fn_map[metric](test_data, idxs, jump)\n",
    "            best_test_metric = 0.\n",
    "            for val_metric, test_metric in zip(val_metrics, test_metrics):\n",
    "                if val_metric == best_val_metric:\n",
    "                    best_test_metric = max(test_metric, best_test_metric)\n",
    "            scores.append(best_test_metric)\n",
    "        test_accs[-1].append(scores)\n",
    "\n",
    "bplot_data = [None] * (2*len(index))\n",
    "bplot_data[::2] = test_accs[0]\n",
    "bplot_data[1::2] = test_accs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 1, figsize=(20, 8))\n",
    "bplot = axs.boxplot(bplot_data, patch_artist=True)\n",
    "colors = ['lightblue', 'lightgreen']\n",
    "for i, patch in enumerate(bplot['boxes']):\n",
    "    patch.set_facecolor(colors[i%2])\n",
    "fontdict = {'family':'serif', 'color':'black', 'weight':'normal', 'size':14}\n",
    "axs.set_ylabel('Accuracy (%)', fontdict=fontdict, labelpad=12)\n",
    "plt.xticks(ticks=np.arange(2.5, 23, 2), labels=index, rotation=15, **fontdict)\n",
    "dx = -45/72; dy = 0/72\n",
    "offset = mlp.transforms.ScaledTranslation(dx, dy, fig.dpi_scale_trans)\n",
    "for label in axs.xaxis.get_majorticklabels():\n",
    "    label.set_transform(label.get_transform() + offset)\n",
    "axs.grid(which='major', color='#AAAAAA', linewidth=0.8)\n",
    "acc = mlp.patches.Patch(color='lightblue', label='AAAI-Constraint')\n",
    "acc_a = mlp.patches.Patch(color='lightgreen', label='AAAI-Constraint-Appended')\n",
    "plt.legend(handles=[acc, acc_a], prop={'family':'serif', 'size': 14})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fakenews",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
